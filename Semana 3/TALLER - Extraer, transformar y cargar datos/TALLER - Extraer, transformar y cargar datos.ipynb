{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfercho100/-PruebaAutomatizacionChoucair/blob/main/Semana%203/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b19a51c305a7880951a3493b81997fa3",
          "grade": false,
          "grade_id": "header-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_YvK_FhkpBet"
      },
      "source": [
        "# Extracción, transformación y carga de datos: `pyspark`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "545f3069f622434931d0aed232433155",
          "grade": false,
          "grade_id": "header-desc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "YFHiOd0rpBew"
      },
      "source": [
        "En este taller extraerás, transformarás y cargarás tablas, haciendo uso de `pyspark` para interactuar con bases de datos relacionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8114ade8404b883dd1c9aec05ee86b2b",
          "grade": false,
          "grade_id": "header-habi-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yxw7WEXopBew"
      },
      "source": [
        "## Habilidades en práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ec1cca48c4858868803cc077c101d691",
          "grade": false,
          "grade_id": "header-habi",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "neMVR6lypBex"
      },
      "source": [
        "Al realizar este taller podrás revisar tu progreso para:\n",
        "\n",
        "**1.** Extraer y transformar tablas de bases de datos relacionales con operaciones de algebra relacional en `pyspark`. <br>\n",
        "**2.** Crear y cargar tablas en bases de datos relacionales con SQL (_Structured Query Language_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "db386060214c4d5b050c05946b6434c0",
          "grade": false,
          "grade_id": "header-instruc-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "pCqgK9QBpBex"
      },
      "source": [
        "## Instrucciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bbd8a4456a32019e7efba2fbdfe165cb",
          "grade": false,
          "grade_id": "header-instruc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8W9ItoHupBex"
      },
      "source": [
        "En cada uno de los siguientes ejercicios deberás escribir el código solicitado estrictamente en las celdas indicadas para ello, teniendo en cuenta las siguientes recomendaciones:\n",
        "\n",
        "* No crear, eliminar o modificar celdas de este Notebook (salvo lo que se te indique), pues puede verse afectado el proceso de calificación automática.\n",
        "\n",
        "* La calificación se realiza de manera automática con datos diferentes a los proporcionados en este taller. Por consiguiente, tu código debe funcionar para diferentes instancias de cada uno de los ejercicios; una instancia hace referencia a los posibles valores de los parámetros.\n",
        "\n",
        "* La calificación de cada ejercicio depende del valor que retorne la función especificada en su enunciado. Por lo tanto, aunque implementes funciones adicionales, es escencial que utilices los nombres propuestos en los enunciados de los ejercicios para implementar la función definitiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "68d388c33cbffb74323dd75af9db7dea",
          "grade": false,
          "grade_id": "bloque-ejer",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "__fyBLr8pBey"
      },
      "source": [
        "## Ejercicios\n",
        "En la siguente celda encuentras declarados los paquetes necesarios para el desarollo de este taller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "18c57fda61db75e34592731234ca3840",
          "grade": false,
          "grade_id": "import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "55_UkLg4pBey"
      },
      "outputs": [],
      "source": [
        "# Esta celda no es modificable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2d67d627edb28de23f37721adae2e72a",
          "grade": false,
          "grade_id": "bloque-init",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "R4aoMBJ5pBez"
      },
      "source": [
        "En la siguiente celda encuentras la inicialización de una sesión de `pyspark`. Puedes editar la configuración según tu criterio, pero recomendamos mantener los valores predefinidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1fUW-ebqpBez",
        "outputId": "4be6a7f0-59c3-47df-a501-fd5c86a00ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ece7fa85ad0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f985ca3b9406:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Instancia_Taller_PySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Esta celda SÍ es modificable\n",
        "#debemos descargar los dos programas de la siguiente pagina:\n",
        "#https://www.oracle.com/java/technologies/downloads/\n",
        "#https://spark.apache.org/downloads.html\n",
        "#https://github.com/steveloughran/winutils\n",
        "#https://www.youtube.com/watch?v=wt2wM8C2SXA            super util\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .appName(\"Instancia_Taller_PySpark\") \\\n",
        "                    .config(\"spark.sql.warehouse.dir\", \"./Archivos/\") \\\n",
        "                    .enableHiveSupport() \\\n",
        "                    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2e458ce21ed6dabaa4a99cc5f3a0e173",
          "grade": false,
          "grade_id": "bloque-start-over",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WQkGqvQ3pBe0"
      },
      "source": [
        "Si en algún momento deseas restablecer el estado de Spark y borrar las bases de datos, puedes correr el siguiente código. Asegurate de reemplazar la ruta al directorio de tu base de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rtQ-a_SBpBe0"
      },
      "outputs": [],
      "source": [
        "# Esta celda SÍ es modificable\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "# Esta instrucción permite borrar bases de datos persistentes.\n",
        "# Si cambiaste el atributo `spark.sql.warehouse.dir` en la configuración\n",
        "# de la sesión, debes reflejar el cambio en esta instrucción.\n",
        "\n",
        "# shutil.rmtree('./Archivos/<nombre_db>.db/')\n",
        "\n",
        "# Esta instrucción borra la base de datos temporal de PySpark.\n",
        "# shutil.rmtree('./metastore_db/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bee373ad53723a4e570ef8c14e1d3370",
          "grade": false,
          "grade_id": "enun",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1QFcrAOypBe0"
      },
      "source": [
        "Harmonialpes, una empresa dedicada a la distribución al por mayor de harmónicas, ha tenido ventas favorables a lo largo del año. Con el gran volumen de harmónicas, el equipo de ventas se ha dado cuenta de que registrarlas en una hoja de Excel compartida no es una forma viable de hacerle seguimiento al desempeño del negocio y ha decidido migrar a un sistema de bases de datos relacionales. Tú, como experto en analítica y gobierno de datos, has sido encargado con la tarea de esta ambiciosa transformación tecnológica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a16f024f3cc9d68c3ca305c5004d46ef",
          "grade": false,
          "grade_id": "ej1-enun-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QChk8GhspBe0"
      },
      "source": [
        "### Ejercicio 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "72ac04a4f9197d227ec6838a987963ef",
          "grade": false,
          "grade_id": "ej1-enun",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5kGVTN8lpBe0"
      },
      "source": [
        "En la siguiente celda encuentras declarada la ruta relativa al archivo de Excel que almacena los datos de todas las órdenes de Harmonialpes, cada una con el respectivo cliente que la realiza y el agente de ventas encargado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5d9e333329947a24e35e1a4c4512e478",
          "grade": false,
          "grade_id": "ej1-data",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "oyoEvojmpBe0"
      },
      "outputs": [],
      "source": [
        "# No modifiques esta celda\n",
        "\n",
        "ruta = r\"Archivos/tabla_ventas.xlsx\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "74cb61eaf7d95556f8abc4f05a34b5f4",
          "grade": false,
          "grade_id": "ej1-task",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5uu19jlVpBe1"
      },
      "source": [
        "Implementa una función llamada `xlsx_a_dataframe` que reciba por parámetro una cadena de texto como la declarada en la celda anterior y que retorne un `DataFrame` de `pyspark`, resultado de leer el archivo. Debes revisar meticulósamente qué campos deben ser de qué tipo, ya que vas a encontrar valores numéricos, fechas y cadenas de texto.\n",
        "# Nueva sección\n",
        "Puedes utilizar métodos de `pandas` para leer los datos, pero no para otros ejercicios del taller.\n",
        "\n",
        "La función debe retornar un `DataFrame` de `pyspark`.\n",
        "\n",
        "Ejecuta tu función con la ruta definida como argumento y guarda el resultado en una variable global llamada `spark_ventas_df`."
      ]
    },
    {
      "source": [
        "def xlsx_a_dataframe(ruta):\n",
        "    \"\"\"\n",
        "    Función que convierte un archivo .xlsx a un DataFrame de PySpark.\n",
        "    :param ruta: Ruta del archivo .xlsx\n",
        "    :return: DataFrame de PySpark\n",
        "    \"\"\"\n",
        "    # Leer el archivo .xlsx usando pandas\n",
        "    # If running in Google Colab, prepend '/content/' to the file path\n",
        "    # if 'google.colab' in str(get_ipython()):\n",
        "    #     ruta = '/content/' + ruta\n",
        "\n",
        "    df_pandas = pd.read_excel(ruta, engine='openpyxl')\n",
        "\n",
        "    # Convertir las columnas de fecha al formato correcto\n",
        "    date_cols = df_pandas.select_dtypes(include=['datetime64']).columns\n",
        "    for col in date_cols:\n",
        "        df_pandas[col] = df_pandas[col].dt.to_pydatetime()\n",
        "\n",
        "    # Convertir el DataFrame de pandas a un DataFrame de PySpark\n",
        "    spark_df = SparkSession.builder.getOrCreate().createDataFrame(df_pandas)\n",
        "\n",
        "    return spark_df\n",
        "\n",
        "# Ejecutar la función y guardar el resultado en la variable global\n",
        "# Replace with the actual absolute path to your file if it's not in the same directory as the notebook:\n",
        "spark_ventas_df = xlsx_a_dataframe(\"Archivos/tabla_ventas.xlsx\") # or \"/your_actual_path/Archivos/tabla_ventas.xlsx\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NiRIAIu-J39I",
        "outputId": "8285c807-9dcb-4ed3-e57b-cb049cb7a4d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-d3027ae1be9b>:17: FutureWarning: The behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n",
            "  df_pandas[col] = df_pandas[col].dt.to_pydatetime()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "35b6c5fcff66986c862b7249133d7f23",
          "grade": true,
          "grade_id": "ej1_test",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8AiBu5OlpBe1",
        "outputId": "6839cd53-e758-4465-c729-d1c925f46f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-d3027ae1be9b>:17: FutureWarning: The behavior of DatetimeProperties.to_pydatetime is deprecated, in a future version this will return a Series containing python datetime objects instead of an ndarray. To retain the old behavior, call `np.array` on the result\n",
            "  df_pandas[col] = df_pandas[col].dt.to_pydatetime()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Felicidades, realizaste este ejercicio correctamente.\n"
          ]
        }
      ],
      "source": [
        "## AUTO-CALIFICADOR\n",
        "\n",
        "# Base variables\n",
        "import pyspark\n",
        "import datetime\n",
        "\n",
        "ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
        "\n",
        "try:\n",
        "    # Caso 1: no existe la función.\n",
        "    try:\n",
        "        xlsx_a_dataframe\n",
        "        assert callable(xlsx_a_dataframe)\n",
        "    except:\n",
        "        raise NotImplementedError(\"No existe una función llamada xlsx_a_dataframe.\",)\n",
        "\n",
        "    # Caso 2: la función es interrumpida por errores durante su ejecución.\n",
        "    try:\n",
        "        resultado = xlsx_a_dataframe(ruta)\n",
        "    except:\n",
        "        raise RuntimeError(\"Tu función produce un error al ejecutarse.\")\n",
        "\n",
        "    # Caso 3: no retorna un DataFrame.\n",
        "    assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
        "\n",
        "    # Caso 4: retorna un dataframe con cantidad de columnas errada\n",
        "    assert len(resultado.columns) == 23, \"Tu función retorna un DataFrame con cantidad de columnas errada.\"\n",
        "\n",
        "    # Caso 5: devuelve un dataframe con cantidad de filas errada\n",
        "    assert resultado.count() == 35, \"Tu función retorna un DataFrame con cantidad de filas errada.\"\n",
        "\n",
        "    # Caso 6: retorna valores no acertados\n",
        "    expected_first_row = [200131, 900, 150, datetime.datetime(2008, 8, 26, 0, 0), 'SOD', 'C00012', 'Steven', 'San Jose', 'San Jose', 'USA', 1, 5000, 7000, 9000, 3000, 'KRFYGJK', 'A012', 'A012', 'Lucida', 'San Jose', 0.12, '044-52981425', 'United States']\n",
        "    expected_last_row = [200124, 500, 100, datetime.datetime(2008, 6, 20, 0, 0), 'SOD', 'C00017', 'Srinivas', 'Bangalore', 'Bangalore', 'India', 2, 8000, 4000, 3000, 9000, 'AAAAAAB', 'A007', 'A007', 'Ramasundar', 'Bangalore', 0.15, '077-25814763', 'India']\n",
        "\n",
        "    assert list(resultado.head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame con valores distintos a los esperados.\"\n",
        "    assert list(resultado.tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame con valores distintos a los esperados.\"\n",
        "\n",
        "    # Caso 7: no guarda el resultado en la variable indicada\n",
        "    try:\n",
        "        spark_ventas_df\n",
        "        assert isinstance(spark_ventas_df, pyspark.sql.dataframe.DataFrame)\n",
        "    except:\n",
        "        raise NotImplementedError(\"No existe un DataFrame llamado spark_ventas_df.\",)\n",
        "\n",
        "except:\n",
        "    # Restaurar variable\n",
        "    ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    # Restaurar variable\n",
        "    ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
        "\n",
        "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ffbdbd172d21f55d17e3780bb26a2843",
          "grade": false,
          "grade_id": "ej2-enun-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EKPFzxrUpBe1"
      },
      "source": [
        "### Ejercicio 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "981713ab79c2fd4216e57d65da4b0347",
          "grade": false,
          "grade_id": "ej2-task",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8741j7RwpBe1"
      },
      "source": [
        "La tabla de ventas, en su estado actual, incluye información del cliente y el agente de ventas para cada orden y, por lo tanto, tiene una dimensión que es incómoda de manejar para los usuarios de los datos. También, cada cliente y cada agente está registrado en una o más ventas, lo que quiere decir que hay mucha información redundante en la tabla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6ce8d4156dbd1a4d0bb74097eb6b1638",
          "grade": false,
          "grade_id": "ej2-task-cont1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2fqEoeoJpBe1"
      },
      "source": [
        "Haciendo uso de la variable que definiste, `spark_ventas_df`, crea una función llamada `desagregar_df` que reciba por parámetro un `DataFrame` con los mismos campos de la variable `spark_ventas_df` y retorne una tupla con tres nuevos `DataFrame` de `pyspark`. Los `DataFrame` deben contener las combinaciones únicas existentes de las columnas de cada categoría (orden, cliente, agente)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a6abeb25969ba54b7d38cc52dccb51a6",
          "grade": false,
          "grade_id": "ej2-task-cont2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "po-Px1AApBe1"
      },
      "source": [
        "Almacena cada `DataFrame` del resultado de la función `desagregar_df` en las siguientes variables globales, según su descripción:\n",
        "\n",
        "- `spark_ordenes_df`: las columnas que describen las órdenes de compra. Estas columnas tienen nombres de la forma `\"Order_<campo>\"`. <br><br>\n",
        "    - La columna `\"Order_Number\"` define individualmente cada orden de compra.<br><br>\n",
        "    - Para poder no perder información acerca de qué cliente realizó cada orden, debemos incluir en este `DataFrame` la columna `Customer_Code`.<br><br>\n",
        "    - Para poder no perder información acerca de qué agente estuvo encargado de cada orden, debemos incluir en este `DataFrame` la columna `Agent_Code`.<br><br>\n",
        "    - Este `DataFrame` no puede contener información adicional de los clientes ni de los agentes.<br><br>\n",
        "\n",
        "- `spark_clientes_df`: las columnas que describen a los clientes. Estas columnas tienen nombres de la forma `\"Customer_<campo>\"`. <br><br>\n",
        "    - La columna `\"Customer_Code\"` define individualmente a cada cliente.<br><br>\n",
        "    - De cada cliente se encarga un único agente. Con el fin de respetar la relación de negocios entre los agentes y sus respectivos clientes, debemos incluir en este `DataFrame` la columna `Agent_Code`.<br><br>\n",
        "    - Este `DataFrame` no puede contener información adicional de las ordenes ni de los agentes.<br><br>\n",
        "    \n",
        "- `spark_agentes_df`: las columnas que describen a los agentes. Estas columnas tienen nombres de la forma `\"Agent_<campo>\"`. <br><br>\n",
        "    - La columna `\"Agent_Code\"` define individualmente a cada agente.<br><br>\n",
        "    - Este `DataFrame` no puede contener información adicional de las ordenes ni de los clientes.\n",
        "    \n",
        "Asegúrate de ordenar las tablas por la columna que define los registros individualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a196bd9e0c9301e8cc99e0a1fe1e7d1b",
          "grade": false,
          "grade_id": "ej2-task-ayuda",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aEaY2h2hpBe1"
      },
      "source": [
        "Ayuda: puedes utilizar el método `DataFrame.colRegex`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a340755d579b5734896ce85bc57b92b1",
          "grade": false,
          "grade_id": "ej2_sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JpfXPEDbpBe2",
        "outputId": "e28bec3f-6a13-4258-f1e0-71f63b3076be"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o97.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (LAPTOP-75HTOMDH executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\braya\\OneDrive - universidadean.edu.co\\Escritorio\\UniversidadDeLosAndes\\Laboratorio_Computacional_de_Analytics\\Semana6\\Taller9\\TALLER - Extraer, transformar y cargar datos.ipynb Celda 26\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m spark_ordenes_df,spark_clientes_df,spark_agentes_df\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m spark_ordenes_df,spark_clientes_df,spark_agentes_df\u001b[39m=\u001b[39mdesagregar_df(spark_ventas_df)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m spark_ordenes_df\u001b[39m.\u001b[39;49mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m spark_clientes_df\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m spark_agentes_df\u001b[39m.\u001b[39mshow()\n",
            "File \u001b[1;32mc:\\Users\\braya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\braya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mc:\\Users\\braya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\braya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o97.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1) (LAPTOP-75HTOMDH executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:705)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:749)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:673)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:615)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:572)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:530)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 31 more\r\n"
          ]
        }
      ],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2df185acc5899a0e69bbb64e8c44403a",
          "grade": true,
          "grade_id": "ej2_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZJSswqN0pBe2"
      },
      "outputs": [],
      "source": [
        "## AUTO-CALIFICADOR\n",
        "\n",
        "# Base variables\n",
        "import pyspark\n",
        "import datetime\n",
        "\n",
        "# Caso 1: no existe la función.\n",
        "try:\n",
        "    desagregar_df\n",
        "    assert callable(desagregar_df)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe una función llamada desagregar_df.\",)\n",
        "\n",
        "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
        "try:\n",
        "    resultado = desagregar_df(spark_ventas_df)\n",
        "except:\n",
        "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")\n",
        "\n",
        "# Caso 3: no retorna una tupla.\n",
        "assert isinstance(resultado, tuple), f\"Tu función debe retornar un objeto de tipo '{tuple.__name__}'.\"\n",
        "\n",
        "# Caso 4: no retorna objetos tipo DataFrame en la tupla.\n",
        "for i in range(3):\n",
        "    assert isinstance(resultado[i], pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar una {tuple.__name__} con elementos de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
        "\n",
        "# Caso 5: retorna un dataframe con cantidad de columnas errada\n",
        "assert len(resultado[0].columns) == 7, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "assert len(resultado[1].columns) == 12, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "assert len(resultado[2].columns) == 6, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "\n",
        "# Caso 6: devuelve un dataframe con cantidad de filas errada\n",
        "assert resultado[0].count() == 34, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
        "assert resultado[1].count() == 25, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
        "assert resultado[2].count() == 12, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
        "\n",
        "# Caso 7: los dataframe no contienen las columnas debidas\n",
        "assert \"Agent_Code\" in resultado[0].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "assert \"Customer_Code\" in resultado[0].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "assert sum(i[:len(\"Order_\")] == \"Order_\" for i in resultado[0].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "\n",
        "assert \"Agent_Code\" in resultado[1].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "assert sum(i[:len(\"Customer_\")] == \"Customer_\" for i in resultado[1].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "\n",
        "assert sum(i[:len(\"Agent_\")] == \"Agent_\" for i in resultado[2].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "\n",
        "# Caso 8: retorna valores no acertados\n",
        "expected_first_row = [200100, 1000, 600, datetime.datetime(2008, 8, 1, 0, 0), 'SOD', 'A003', 'C00013']\n",
        "expected_last_row = [200135, 2000, 800, datetime.datetime(2008, 9, 16, 0, 0), 'SOD', 'A010', 'C00007']\n",
        "assert list(resultado[0].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "assert list(resultado[0].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "\n",
        "expected_first_row = ['C00001', 'Micheal', 'New York', 'New York', 'USA', 2, 3000, 5000, 2000, 6000, 'CCCCCCC', 'A008']\n",
        "expected_last_row = ['C00025', 'Ravindran', 'Bangalore', 'Bangalore', 'India', 2, 5000, 7000, 4000, 8000, 'AVAVAVA', 'A011']\n",
        "assert list(resultado[1].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "assert list(resultado[1].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "\n",
        "expected_first_row = ['A001', 'Subbarao', 'Bangalore', 0.14, '077-12346674', 'India']\n",
        "expected_last_row = ['A012', 'Lucida', 'San Jose', 0.12, '044-52981425', 'United States']\n",
        "assert list(resultado[2].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "assert list(resultado[2].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "\n",
        "# Caso 9: no existen las variables indicadas\n",
        "try:\n",
        "    spark_ordenes_df\n",
        "    assert isinstance(spark_ordenes_df, pyspark.sql.dataframe.DataFrame)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe un DataFrame llamado spark_ordenes_df.\",)\n",
        "\n",
        "try:\n",
        "    spark_clientes_df\n",
        "    assert isinstance(spark_clientes_df, pyspark.sql.dataframe.DataFrame)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe un DataFrame llamado spark_clientes_df.\",)\n",
        "\n",
        "try:\n",
        "    spark_agentes_df\n",
        "    assert isinstance(spark_agentes_df, pyspark.sql.dataframe.DataFrame)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe un DataFrame llamado spark_agentes_df.\",)\n",
        "\n",
        "# Caso 10: no guarda el resultado en las variables acertadas\n",
        "assert spark_ordenes_df.collect() == resultado[0].collect(), \"La variable spark_ordenes_df no guarda el primer DataFrame de la tupla.\"\n",
        "assert spark_clientes_df.collect() == resultado[1].collect(), \"La variable spark_clientes_df no guarda el segundo DataFrame de la tupla.\"\n",
        "assert spark_agentes_df.collect() == resultado[2].collect(), \"La variable spark_agentes_df no guarda el tercer DataFrame de la tupla.\"\n",
        "\n",
        "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4412615a4912cb1aa448d225705f3993",
          "grade": false,
          "grade_id": "ej3-enun-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6YOCciKLpBe2"
      },
      "source": [
        "### Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1deb7ba8ede390c268ed982c519b0310",
          "grade": false,
          "grade_id": "ej3-enun",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SSYsoH1YpBe2"
      },
      "source": [
        "En preparación para la transformación tecnológica, Harmonialpes ha estado recopilando más y más información para almacenar en bases de datos y aprovechar al máximo la inversión en infraestructura y tecnología. En aras de poder acomodar nuevos esquemas de datos y gran diversidad de información, han pedido una función que pueda crear nuevas bases de datos, en las cuales puedan almacenar permanentemente nuevas tablas y sus relaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "534db846b7c912b724f38ccf3c14ff53",
          "grade": false,
          "grade_id": "ej3-enun-cont",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LB4NVkM-pBe2"
      },
      "source": [
        "En la siguiente celda encuentras declarada una lista de cadenas de texto, cada una con el nombre de una base de datos que necesitan para almacenar su información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e3be1c574a8bd8eb43e89ac6f73cdc4f",
          "grade": false,
          "grade_id": "ej3-data",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ONg3c9A0pBe2"
      },
      "outputs": [],
      "source": [
        "# No modifiques esta celda\n",
        "\n",
        "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cd86780df60c8ca6e3aaf2f56af47323",
          "grade": false,
          "grade_id": "ej3-task",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1x2sN42dpBe3"
      },
      "source": [
        "Implementa una función llamada `crear_varios_db` que reciba por parámetro una lista de cadenas de texto como la declarada en la celda anterior y que retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener una única columna llamada `\"namespace\"` cuyos valores sean las bases de datos en el directorio de archivos (no debe incluir `default`). La función no debe crear las bases de datos que ya existan en el directorio.\n",
        "\n",
        "La función debe retornar un `DataFrame` de `pyspark`.\n",
        "\n",
        "Ejecuta tu función con la lista de nombres a crear definida como argumento y guarda el resultado en una variable global llamada `spark_bases_de_datos_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ff408280ec24f14ab5c0cd9d80b01ec7",
          "grade": false,
          "grade_id": "ej3_sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "HwF1nW-1pBe3",
        "outputId": "7fbf7c4c-3bf5-43a0-991c-7722c64c7732"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\braya\\OneDrive - universidadean.edu.co\\Escritorio\\UniversidadDeLosAndes\\Laboratorio_Computacional_de_Analytics\\Semana6\\Taller9\\TALLER - Extraer, transformar y cargar datos.ipynb Celda 33\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data \u001b[39m=\u001b[39m [[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nombres_a_crear]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m spark\u001b[39m.\u001b[39mcreateDataFrame(data\u001b[39m=\u001b[39mdata, schema\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnamespace\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m spark_bases_de_datos_df \u001b[39m=\u001b[39m crear_varios_db(nombres_a_crear)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m spark_bases_de_datos_df\u001b[39m.\u001b[39mshow()\n",
            "\u001b[1;32mc:\\Users\\braya\\OneDrive - universidadean.edu.co\\Escritorio\\UniversidadDeLosAndes\\Laboratorio_Computacional_de_Analytics\\Semana6\\Taller9\\TALLER - Extraer, transformar y cargar datos.ipynb Celda 33\u001b[0m in \u001b[0;36mcrear_varios_db\u001b[1;34m(lista)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcrear_varios_db\u001b[39m(lista:\u001b[39mlist\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#cuyos valores sean las bases de datos en el directorio de archivos (no debe incluir default)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39mCREATE DATABASE IF NOT EXISTS \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(lista[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data \u001b[39m=\u001b[39m [[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nombres_a_crear]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/braya/OneDrive%20-%20universidadean.edu.co/Escritorio/UniversidadDeLosAndes/Laboratorio_Computacional_de_Analytics/Semana6/Taller9/TALLER%20-%20Extraer%2C%20transformar%20y%20cargar%20datos.ipynb#Y101sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m spark\u001b[39m.\u001b[39mcreateDataFrame(data\u001b[39m=\u001b[39mdata, schema\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnamespace\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ae9d683b8da1c4ec8fb65293271b6f4e",
          "grade": true,
          "grade_id": "ej3_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "r17UbKZJpBe3"
      },
      "outputs": [],
      "source": [
        "## AUTO-CALIFICADOR\n",
        "\n",
        "# Base variables\n",
        "import pyspark\n",
        "\n",
        "\n",
        "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
        "\n",
        "try:\n",
        "    # Caso 1: no existe la función.\n",
        "    try:\n",
        "        crear_varios_db\n",
        "        assert callable(crear_varios_db)\n",
        "    except:\n",
        "        raise NotImplementedError(\"No existe una función llamada crear_varios_db.\",)\n",
        "\n",
        "    # Caso 2: la función es interrumpida por errores durante su ejecución.\n",
        "    try:\n",
        "        resultado = crear_varios_db(nombres_a_crear)\n",
        "        nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
        "    except:\n",
        "        raise RuntimeError(\"Tu función produce un error al ejecutarse.\")\n",
        "\n",
        "    # Caso 3: no retorna un DataFrame.\n",
        "    assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
        "\n",
        "    # Caso 4: retorna un dataframe con cantidad de columnas errada\n",
        "    assert len(resultado.columns) == 1, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "\n",
        "    # Caso 5: devuelve un dataframe con cantidad de filas errada\n",
        "    assert resultado.count() >= len(nombres_a_crear), \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
        "\n",
        "    # Caso 6: los dataframe no contienen las columnas debidas\n",
        "    assert \"namespace\" in resultado.columns, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "\n",
        "    # Caso 7: retorna valores no acertados\n",
        "    assert resultado.withColumn(\"was_requested\", functions.col(\"namespace\") \\\n",
        "                                         .isin(nombres_a_crear) \\\n",
        "                                         .cast(\"long\")) \\\n",
        "                                         .agg(functions.sum(\"was_requested\")).collect()[0][0] == len(nombres_a_crear), \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegúrate de no tener filas repetidas o faltantes.\"\n",
        "\n",
        "    # Caso 8: no existen las variables indicadas\n",
        "    try:\n",
        "        spark_bases_de_datos_df\n",
        "        assert isinstance(spark_bases_de_datos_df, pyspark.sql.dataframe.DataFrame)\n",
        "    except:\n",
        "        raise NotImplementedError(\"No existe un DataFrame llamado spark_bases_de_datos_df.\",)\n",
        "\n",
        "    # Caso 9: no crea las bases de datos\n",
        "    data = [[i] for i in nombres_a_crear]\n",
        "    assert set(spark.sql(\"SHOW DATABASES;\").collect()).issuperset(set(spark.createDataFrame(data=data, schema=[\"namespace\"]).collect())), \"No has creado todas las bases de datos solicitadas.\"\n",
        "\n",
        "except:\n",
        "    nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
        "    raise\n",
        "\n",
        "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
        "\n",
        "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "688845290fb9e2c43d2b24179a428591",
          "grade": false,
          "grade_id": "ej4-enun-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2eZaXbKrpBe3"
      },
      "source": [
        "### Ejercicio 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "16a4c387295fe299b342ae1e450c86c7",
          "grade": false,
          "grade_id": "ej4-enun",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BG1GKOpypBe3"
      },
      "source": [
        "Ahora que existe la bases de datos `ventas_db`, podemos cargar nuestras tablas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6a46584c6bac360104b1c542f47dcb92",
          "grade": false,
          "grade_id": "ej4-task",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mCi-LIPxpBe3"
      },
      "source": [
        "Implementa una función llamada `cargar_a_ventas_db` que no reciba parámetros, cargue las tablas `spark_ordenes_df`, `spark_clientes_df` y `spark_agentes_df` a la base de datos `ventas_db` y retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener una única columna llamada `\"tableName\"`, cuyos valores sean las tablas en la base de datos `ventas_db`. Los nombres de las tablas en la base de datos deben ser los mismos de las variables en `pyspark`. La función no debe crear las tablas que ya existan en la base de datos.\n",
        "\n",
        "La función debe retornar un `DataFrame` de `pyspark`.\n",
        "\n",
        "Ejecuta tu función y guarda el resultado en una variable global llamada `spark_tablas_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d5c1f1559b445cab02401285481cc494",
          "grade": false,
          "grade_id": "ej4_sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "iockZZkqpBe3"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0b1e81ffc3d21803c84e0a4b8f5c6db2",
          "grade": true,
          "grade_id": "ej4_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "pVGRQWxkpBe3"
      },
      "outputs": [],
      "source": [
        "## AUTO-CALIFICADOR\n",
        "\n",
        "# Base variables\n",
        "import pyspark\n",
        "\n",
        "# Caso 1: no existe la función.\n",
        "try:\n",
        "    cargar_a_ventas_db\n",
        "    assert callable(cargar_a_ventas_db)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe una función llamada cargar_a_ventas_db.\",)\n",
        "\n",
        "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
        "try:\n",
        "    resultado = cargar_a_ventas_db()\n",
        "except:\n",
        "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")\n",
        "\n",
        "# Caso 3: no retorna un DataFrame.\n",
        "assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
        "\n",
        "# Caso 4: retorna un dataframe con cantidad de columnas errada\n",
        "assert len(resultado.columns) == 1, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "\n",
        "# Caso 5: devuelve un dataframe con cantidad de filas errada\n",
        "assert resultado.count() == 3, \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes y que tu base de datos tenga las tablas estrictamente necesarias.\"\n",
        "\n",
        "# Caso 6: los dataframe no contienen las columnas debidas\n",
        "assert \"tableName\" in resultado.columns, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
        "\n",
        "# Caso 7: retorna valores no acertados\n",
        "tablas = [\"spark_ordenes_df\", \"spark_clientes_df\", \"spark_agentes_df\"]\n",
        "assert resultado.withColumn(\"was_requested\", functions.col(\"tableName\") \\\n",
        "                                     .isin(tablas) \\\n",
        "                                     .cast(\"long\")) \\\n",
        "                                     .agg(functions.sum(\"was_requested\")).collect()[0][0] == len(tablas), \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegúrate de no tener filas repetidas o faltantes.\"\n",
        "\n",
        "# Caso 8: no existen las variables indicadas\n",
        "try:\n",
        "    spark_tablas_df\n",
        "    assert isinstance(spark_tablas_df, pyspark.sql.dataframe.DataFrame)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe un DataFrame llamado spark_tablas_df.\",)\n",
        "\n",
        "# Caso 9: no crea las tablas\n",
        "data = [[i] for i in tablas]\n",
        "assert set(spark.sql(\"SHOW TABLES FROM ventas_db;\").select(\"tableName\").collect()) == set(spark.createDataFrame(data=data, schema=[\"tableName\"]).collect()), \"Las tablas creadas no son estrictamente las solicitadas.\"\n",
        "\n",
        "\n",
        "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7e20599200d070c1b16e470375456ae9",
          "grade": false,
          "grade_id": "ej5-enun-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7eAMSKIkpBe4"
      },
      "source": [
        "### Ejercicio 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b9b4545ba47a266a74c0f2dc36a406aa",
          "grade": false,
          "grade_id": "ej5-enun",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8Dive25KpBe4"
      },
      "source": [
        "Al equipo de ventas le interesa saber cuáles clientes corresponden a cada agente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b753b5747cd1331795b84d527c44a93f",
          "grade": false,
          "grade_id": "ej5-task",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "092XDpl9pBe4"
      },
      "source": [
        "Implementa una función llamada `consulta_join` que no reciba parámetros, que retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener todas las columnas de las tablas `spark_agentes_df` y `spark_clientes_df`, de la base de datos `ventas_db`, de manera que para cada agente se reporten los clientes que le corresponden o `NULL` si no le corresponde ninguno. Debes ordenar el `DataFrame` por la columna `Customer_Code`.\n",
        "\n",
        "La función debe retornar un `DataFrame` de `pyspark`.\n",
        "\n",
        "Ejecuta tu función y guarda el resultado en una variable global llamada `spark_consulta_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "61cc4eaf8ab84bf13a779b041002a91b",
          "grade": false,
          "grade_id": "ej5_sol",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Om7bcMf2pBe4"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a97f68395af6a537ed3cb398391bafb8",
          "grade": true,
          "grade_id": "ej5_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1qgs5QhMpBe5"
      },
      "outputs": [],
      "source": [
        "## AUTO-CALIFICADOR\n",
        "\n",
        "# Base variables\n",
        "import pyspark\n",
        "\n",
        "# Caso 1: no existe la función.\n",
        "try:\n",
        "    consulta_join\n",
        "    assert callable(consulta_join)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe una función llamada consulta_join.\",)\n",
        "\n",
        "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
        "try:\n",
        "    resultado = consulta_join()\n",
        "except:\n",
        "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")\n",
        "\n",
        "# Caso 3: no retorna un DataFrame.\n",
        "assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
        "\n",
        "# Caso 4: retorna un dataframe con cantidad de columnas errada\n",
        "assert len(resultado.columns) == 17, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
        "\n",
        "# Caso 5: devuelve un dataframe con cantidad de filas errada\n",
        "assert resultado.count() == 25, \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
        "\n",
        "# Caso 6: los dataframe no contienen las columnas debidas\n",
        "columnas = ['Agent_Code', 'Agent_Name', 'Agent_Working_Area', 'Agent_Commission', 'Agent_Phone_No', 'Agent_Country', 'Customer_Code', 'Customer_Name', 'Customer_City', 'Customer_Working_Area', 'Customer_Country', 'Customer_Grade', 'Customer_Opening_Amount', 'Customer_Receive_Amount', 'Customer_Payment_Amount', 'Customer_Outstanding_Amount', 'Customer_Phone_No']\n",
        "assert resultado.columns == columnas, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar ni reordenar las columnas.\"\n",
        "\n",
        "# Caso 7: retorna valores no acertados\n",
        "expected_first_row = ['A008', 'Alford', 'New York', 0.12, '044-25874365', 'United States', 'C00001', 'Micheal', 'New York', 'New York', 'USA', 2, 3000, 5000, 2000, 6000, 'CCCCCCC']\n",
        "expected_last_row = ['A011', 'Ravi Kumar', 'Bangalore', 0.15, '077-45625874', 'India', 'C00025', 'Ravindran', 'Bangalore', 'Bangalore', 'India', 2, 5000, 7000, 4000, 8000, 'AVAVAVA']\n",
        "\n",
        "assert list(resultado.head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "assert list(resultado.tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
        "\n",
        "# Caso 8: no existen las variables indicadas\n",
        "try:\n",
        "    spark_consulta_df\n",
        "    assert isinstance(spark_consulta_df, pyspark.sql.dataframe.DataFrame)\n",
        "except:\n",
        "    raise NotImplementedError(\"No existe un DataFrame llamado spark_consulta_df.\",)\n",
        "\n",
        "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c79f24369aa0cc741bb46674c1777fd1",
          "grade": false,
          "grade_id": "refs-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "geYh0_KPpBe5"
      },
      "source": [
        "## Referencias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ef46fdef3658a1416714291615da4f23",
          "grade": false,
          "grade_id": "refs",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "olPDPUwRpBe5"
      },
      "source": [
        "SparkBy{Examples}. Spark with Python (PySpark) Tutorial For Beginners. Recuperado el 12 de Agosto de 2022 de:\n",
        "https://sparkbyexamples.com/pyspark-tutorial/\n",
        "\n",
        "Apache PySpark. pyspark.sql.DataFrame. Recuperado el 12 de Agosto de 2022 de: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html\n",
        "\n",
        "w3resource. SQL Table. Recuperado el 20 de Agosto de 2022 de: https://www.w3resource.com/sql/sql-table.php"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3e40d2a9d99b7552b736f47b3289cf1f",
          "grade": false,
          "grade_id": "creds-titulo",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "OoOgCez4pBe5"
      },
      "source": [
        "## Créditos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "60a09939fed82859b9b28289a0f8e69f",
          "grade": false,
          "grade_id": "creds",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vbq_KlPZpBe5"
      },
      "source": [
        "**Autor(es)**: Alejandro Mantilla Redondo, Diego Alejandro Cely\n",
        "\n",
        "**Fecha última actualización:** 14/09/2022"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "8b8193576751691e149fe4b02bd81acb44aebb7039077c15be9d1f0082e2f63d"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}